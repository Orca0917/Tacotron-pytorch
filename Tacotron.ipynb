{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as taf\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054475b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 다운로드\n",
    "dataset = torchaudio.datasets.LJSPEECH('.', download=True)\n",
    "\n",
    "# 텍스트를 원핫인코딩해주는 전처리기 가져오기\n",
    "text_preprocess = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604efed5",
   "metadata": {},
   "source": [
    "샘플 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 데이터의 텍스트, 전처리된 텍스트\n",
    "sample_text = dataset[0][3]\n",
    "sample_phoneme, _ = text_preprocess(sample_text)\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"Text:\", sample_text)\n",
    "print(\"Character sequence:\", sample_phoneme)\n",
    "\n",
    "# 음성 확인\n",
    "Audio(dataset[0][0], rate=24000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe22ed2",
   "metadata": {},
   "source": [
    "## Tacotron dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45596a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams:\n",
    "    # Audio preprocessing\n",
    "    preemphasis = 0.97\n",
    "    n_fft = 2048\n",
    "    window = 'hann'\n",
    "    frame_length_ms = 50\n",
    "    frame_shift_ms = 12.5\n",
    "    sample_rate = 24000\n",
    "    ref_level_db = 20\n",
    "    max_level_db = 100\n",
    "\n",
    "    # min_level_db = -100\n",
    "    # max_level_db = 100\n",
    "    \n",
    "    n_mels = 80\n",
    "    win_length = int(round(frame_length_ms * sample_rate / 1000))\n",
    "    hop_length = int(round(frame_shift_ms * sample_rate / 1000))\n",
    "    n_phonemes = 70\n",
    "\n",
    "    # model\n",
    "    reduction_factor = 2\n",
    "    character_embedding_dim = 256\n",
    "    encoder_cbhg_k = 16\n",
    "    encoder_cbhg_dim = 128\n",
    "    encoder_conv1d_projection = [128, 128]\n",
    "    encoder_highway = [128, 128, 128, 128]\n",
    "    encoder_bidirectional_gru = 128\n",
    "    encoder_prenet = [256, 128]\n",
    "    decoder_prenet = [256, 128]\n",
    "    decoder_cbhg_k = 8\n",
    "    decoder_cbhg_dim = 128\n",
    "    decoder_conv1d_projection = [256, 80]\n",
    "    decoder_highway = [128, 128, 128, 128]\n",
    "    decoder_bidirectional_gru = 128\n",
    "    \n",
    "    attention_rnn_dim = 256\n",
    "    decoder_rnn_dim = 256\n",
    "\n",
    "    # training\n",
    "    batch_size = 16 # all sequences are padded to max length\n",
    "    max_decoding_timestep = 200\n",
    "    learning_rate = 0.001\n",
    "\n",
    "hp = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as taf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TacotronDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    https://github.com/Kyubyong/tacotron/blob/master/utils.py#L21\n",
    "    https://github.com/ttaoREtw/Tacotron-pytorch/blob/master/src/utils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataset = torchaudio.datasets.LJSPEECH('.', download=True)\n",
    "        self.text_preprocess = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # get data\n",
    "        y, sr, _, text = self.dataset[idx]\n",
    "\n",
    "        # text preprocess\n",
    "        text, _ = self.text_preprocess(text)\n",
    "\n",
    "        # audio preprocess\n",
    "        lin, mel = self.audio_preprocess(y, sr)\n",
    "\n",
    "        # (text_len), (2048, len), (n_mel, mel_len)\n",
    "        return text.squeeze(), lin, mel\n",
    "    \n",
    "    # Preemphasis\n",
    "    def preemphasis(self, wav):\n",
    "        return taf.preemphasis(wav, hp.preemphasis)\n",
    "    \n",
    "    # Short Time Frourier Transform\n",
    "    def _stft(self, x):\n",
    "        return librosa.stft(x.numpy().squeeze(), n_fft=hp.n_fft, hop_length=hp.hop_length, \n",
    "                            win_length=hp.win_length, window=hp.window, pad_mode='constant')\n",
    "\n",
    "    # Spectrogram\n",
    "    def spectrogram(self, wav):\n",
    "        D = self._stft(self.preemphasis(wav))\n",
    "        S = self._amp_to_db(np.abs(D)) - hp.ref_level_db\n",
    "        return self._normalize(S)\n",
    "    \n",
    "    # Melspectrogram\n",
    "    def melspectrogram(self, wav):\n",
    "        D = self._stft(self.preemphasis(wav))\n",
    "        S = self._amp_to_db(self._linear_to_mel(np.abs(D))) # - hp.ref_level_db\n",
    "        return self._normalize(S)\n",
    "    \n",
    "    # amplitude to decibel\n",
    "    def _amp_to_db(self, x):\n",
    "        return 20 * np.log10(np.maximum(1e-5, x))\n",
    "    \n",
    "    # spectrogram to melspectrogram\n",
    "    def _linear_to_mel(self, mag):\n",
    "        mel_basis = librosa.filters.mel(sr=hp.sample_rate, n_fft=hp.n_fft, n_mels=hp.n_mels)\n",
    "        return np.dot(mel_basis, mag)\n",
    "    \n",
    "    # normalization\n",
    "    def _normalize(self, x):\n",
    "        return np.clip((x - hp.ref_level_db + hp.max_level_db) / hp.max_level_db, 0, 1)\n",
    "        # return np.clip((x - hp.min_level_db) / -hp.min_level_db, 0, 1)\n",
    "        # return np.clip((2 * hp.max_abs_value) * ((x - hp.min_level_db) / (-hp.min_level_db)) - hp.max_abs_value, \n",
    "        #                -hp.max_abs_value, hp.max_abs_value)\n",
    "\n",
    "    def audio_preprocess(self, y, sr):\n",
    "\n",
    "        # trimming\n",
    "        y, _ = librosa.effects.trim(y)\n",
    "\n",
    "        # spectrogram, melspectrogram\n",
    "        spectrogram = self.spectrogram(y)\n",
    "        melspectrogram = self.melspectrogram(y)\n",
    "        \n",
    "        return torch.FloatTensor(spectrogram), torch.FloatTensor(melspectrogram)\n",
    "\n",
    "\n",
    "class TacotronTTSCollate():\n",
    "\n",
    "    def __init__(self):\n",
    "        ...\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        # get decreasing order by text length within batch\n",
    "        text_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(text) for text, _, _ in batch]),\n",
    "            dim=0, descending=True\n",
    "        )\n",
    "\n",
    "        # all zero padded tensor\n",
    "        max_text_len = text_lengths[0]\n",
    "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
    "        text_padded.zero_()\n",
    "\n",
    "        # allocate text to zero padded tensor\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text = batch[ids_sorted_decreasing[i]][0]\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "\n",
    "        # get maximum length of sequence within batch\n",
    "        num_lins = batch[0][1].size(0)\n",
    "        num_mels = batch[0][2].size(0)\n",
    "        max_seq_len = max([lin.size(1) for _, lin, _ in batch])\n",
    "        max_seq_len = max_seq_len + (hp.reduction_factor - max_seq_len % hp.reduction_factor)\n",
    "\n",
    "\n",
    "        # all zero padded tensor\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_seq_len)\n",
    "        mel_padded.zero_()\n",
    "        lin_padded = torch.FloatTensor(len(batch), num_lins, max_seq_len)\n",
    "        lin_padded.zero_()\n",
    "        seq_lengths = torch.LongTensor(len(batch))\n",
    "\n",
    "        \n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            _, lin, mel = batch[ids_sorted_decreasing[i]]\n",
    "            lin_padded[i, :, :lin.size(1)] = lin\n",
    "            mel_padded[i, :, :mel.size(1)] = mel\n",
    "            seq_lengths[i] = lin.size(1)\n",
    "\n",
    "\n",
    "        return (\n",
    "            text_padded,\n",
    "            lin_padded.transpose(1, 2),\n",
    "            mel_padded.transpose(1, 2),\n",
    "            text_lengths,\n",
    "            seq_lengths\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_melspectrogram(mel_pred, mel_targ):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    im1 = ax1.imshow(mel_pred, aspect=\"auto\", origin=\"lower\", interpolation=\"none\")\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "    im2 = ax2.imshow(mel_targ, aspect=\"auto\", origin=\"lower\", interpolation=\"none\")\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f04fad",
   "metadata": {},
   "source": [
    "3.2 Encoder\n",
    "\n",
    "The goal of the encoder is to extract robust sequential representations of text. The input to the encoder is a character sequence, where each character is represented as a one-hot vector and embedded into a continuous vector. We then apply a set of non-linear transformations, collectively called a “pre-net”, to each embedding. We use a bottleneck layer with dropout as the pre-net in this work, which helps convergence and improves generalization. A CBHG module transforms the prenet outputs into the final encoder representation used by the attention module. We found that this CBHG-based encoder not only reduces overfitting, but also makes fewer mispronunciations than a standard multi-layer RNN encoder (see our linked page of audio samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880005a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DBN(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, bias=False, activation=None):\n",
    "        super(Conv1DBN, self).__init__()\n",
    "        self.conv_1d = nn.Conv1d(in_channels=in_ch, out_channels=out_ch, kernel_size=k, padding=k//2, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1d convolution\n",
    "        x = self.conv_1d(x)\n",
    "        \n",
    "        # activation function\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        # batch normalization\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8dcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    \"\"\"\n",
    "    https://github.com/r9y9/tacotron_pytorch/blob/master/tacotron_pytorch/tacotron.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Highway, self).__init__()\n",
    "        self.H = nn.Linear(in_size, out_size)\n",
    "        self.H.bias.data.zero_()\n",
    "        self.T = nn.Linear(in_size, out_size)\n",
    "        self.T.bias.data.fill_(-1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        H = self.relu(self.H(inputs))\n",
    "        T = self.sigmoid(self.T(inputs))\n",
    "        return H * T + inputs * (1.0 - T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b8dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNet(nn.Module):\n",
    "    def __init__(self, ch_emb_dim, hidden_dims):\n",
    "        super(PreNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(ch_emb_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2 fully connected layers : (B, text_len, 128)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cad760",
   "metadata": {},
   "source": [
    "### CBHG\n",
    "\n",
    "CBHG consists of a bank of 1-D convolutional filters, followed by highway networks (Srivastava et al., 2015) and a bidirectional gated recurrent unit (GRU) (Chung et al., 2014) recurrent neural net (RNN).  The input sequence is first convolved with $K$ sets of $1\\text{-D}$ convolutional filters, where the $k$-th set contains $C_k$ filters of width $k\\; (\\text{i.e. } k = 1, 2, . . . , K)$. \n",
    "\n",
    "The convolution outputs are stacked together and further max pooled along time to increase local invariances. Note that we use a stride of 1 to preserve the original time resolution. We further pass the processed sequence to a few fixed-width 1-D convolutions, whose outputs are added with the original input sequence via residual connections (He et al., 2016). Batch normalization (Ioffe & Szegedy, 2015) is used for all convolutional layers.\n",
    "\n",
    "The convolution outputs are fed into a multi-layer highway network to extract high-level features. Finally, we stack a bidirectional GRU RNN on top to extract sequential features from both forward and backward context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c45abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DBank(nn.Module):\n",
    "    def __init__(self, K, cbhg_dim):\n",
    "        super(Conv1DBank, self).__init__()\n",
    "        relu = nn.ReLU()\n",
    "        self.convolutions = nn.ModuleList(\n",
    "            [Conv1DBN(cbhg_dim, cbhg_dim, k, False, relu) \n",
    "             for k in range(1, K + 1)]\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=2//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        text_len = x.size(1)\n",
    "\n",
    "        # convolution 연산을 위해 channel은 1번째 index에 위치해야함\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # PAPER: The convolution outputs are stacked together\n",
    "        x = torch.cat([conv(x)[:, :, :text_len] for conv in self.convolutions], dim=1)\n",
    "\n",
    "        # PAPER: and further max pooled along ...\n",
    "        x = self.maxpool(x)[:, :, :text_len]\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ec91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvProjection(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim):\n",
    "        super(ConvProjection, self).__init__()\n",
    "\n",
    "        # convolution layer dimensions\n",
    "        in_dims = [in_dim] + hidden_dims[:-1]\n",
    "        out_dims = hidden_dims[1:]\n",
    "        activations = [nn.ReLU()] * len(out_dims) + [None]\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dims[-1], out_dim)\n",
    "        self.convolutions = nn.ModuleList(\n",
    "            [Conv1DBN(in_dim, out_dim, k=3, bias=True, activation=act)\n",
    "             for in_dim, out_dim, act in zip(in_dims, out_dims, activations)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # convolution projections\n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "            \n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # match dimension with highway input\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6deb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBHG(nn.Module):\n",
    "    def __init__(self, K, cbhg_dim, proj_dims, highway_dims, gru_dim):\n",
    "        super(CBHG, self).__init__()\n",
    "        self.convolution_bank = Conv1DBank(K, cbhg_dim)\n",
    "        self.convolution_proj = ConvProjection(K * cbhg_dim, proj_dims, highway_dims[0])\n",
    "        self.highways = nn.ModuleList([\n",
    "            Highway(in_dim, out_dim) \n",
    "            for in_dim, out_dim in zip(highway_dims[:-1], highway_dims[1:])])\n",
    "        self.bidirectional_gru = nn.GRU(highway_dims[-1], gru_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual\n",
    "        residual = x\n",
    "\n",
    "        # Conv 1D bank + stacking + maxpool -> [B, text_len, K * 256]\n",
    "        x = self.convolution_bank(x)\n",
    "\n",
    "        # Convolution projection -> [B, text_len, 128]\n",
    "        x = self.convolution_proj(x)\n",
    "\n",
    "        # residual connection\n",
    "        x += residual\n",
    "\n",
    "        # highway layers -> [B, text_len, 128]\n",
    "        for highway in self.highways:\n",
    "            x = highway(x)\n",
    "\n",
    "        # Bidirectional RNN -> [B, text_len, 256]\n",
    "        x, _ = self.bidirectional_gru(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # character embedding\n",
    "        n_phonemes = hparams.n_phonemes\n",
    "        ch_emb_dim = hparams.character_embedding_dim\n",
    "        self.character_embedding = nn.Embedding(n_phonemes, ch_emb_dim)\n",
    "\n",
    "        # prenet\n",
    "        prenet_dim = hparams.encoder_prenet\n",
    "        self.prenet = PreNet(ch_emb_dim, prenet_dim)\n",
    "\n",
    "        # cbhg\n",
    "        encoder_K = hparams.encoder_cbhg_k\n",
    "        encoder_cbhg_dim = hparams.encoder_cbhg_dim\n",
    "        encoder_proj_dim = hparams.encoder_conv1d_projection\n",
    "        highway_dims = hparams.encoder_highway\n",
    "        gru_dim = hparams.encoder_bidirectional_gru\n",
    "        self.cbhg = CBHG(encoder_K, encoder_cbhg_dim, encoder_proj_dim, highway_dims, gru_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # character embedding -> [B, text_len, 128]\n",
    "        x = self.character_embedding(x)\n",
    "\n",
    "        # prenet -> [B, text_len, 128]\n",
    "        x = self.prenet(x)\n",
    "\n",
    "        # cbhg -> [B, text_len, 256]\n",
    "        x = self.cbhg(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e10cb",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "We use a content-based tanh attention decoder (see e.g. Vinyals et al. (2015)), where a stateful recurrent layer produces the attention query at each decoder time step. We concatenate the context vector and the attention RNN cell output to form the input to the decoder RNNs. We use a stack of GRUs with vertical residual connections (Wu et al., 2016) for the decoder.\n",
    "\n",
    "While we could directly predict raw spectrogram, it’s a highly redundant representation for the purpose of learning alignment between speech signal and text. We use 80-band mel-scale spectrogram as the target, though fewer bands or more concise targets such as cepstrum could be used. We use a post-processing network (discussed below) to convert from the seq2seq target to waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, prenet_dim, attn_rnn_hidden_dim):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.gru = nn.GRUCell(prenet_dim + attn_rnn_hidden_dim, attn_rnn_hidden_dim)\n",
    "\n",
    "    def forward(self, prenet_out, attn_out, attn_rnn_hidden):\n",
    "        \n",
    "        # We concatenate the context vector and the attention RNN cell output \n",
    "        x = torch.cat([prenet_out, attn_out], dim=1)\n",
    "\n",
    "        # 1-layer GRU (256 cells)\n",
    "        x = self.gru(x, attn_rnn_hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2de14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer residual GRU (default: 256 cells)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim) # 이거는 차원 맞춰주기용인데, gru1의 입력 차원을 변경할지는 생각해봐야할 듯\n",
    "        self.gru1 = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "        self.gru2 = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, attn_out, gru_out, dec_rnn_hiddens):\n",
    "        # - attnention out  : (B, 256)\n",
    "        # - gru out         : (B, 256)\n",
    "\n",
    "        # match input dimension with linear -> [B, 256]\n",
    "        decoder_in = torch.cat([gru_out, attn_out], dim=-1)\n",
    "        decoder_in = self.linear1(decoder_in)\n",
    "\n",
    "        # first layer residual GRU -> [B, 256]\n",
    "        dec_rnn_hiddens[0] = self.gru1(decoder_in, dec_rnn_hiddens[0])\n",
    "        decoder_in = decoder_in + dec_rnn_hiddens[0]\n",
    "\n",
    "        # second layer residual GRU -> [B, 256]\n",
    "        dec_rnn_hiddens[1] = self.gru2(decoder_in, dec_rnn_hiddens[1])\n",
    "        decoder_out = decoder_in + dec_rnn_hiddens[1]\n",
    "\n",
    "        # [B, 256], [2, B, 256]\n",
    "        return decoder_out, dec_rnn_hiddens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94907b",
   "metadata": {},
   "source": [
    "Attention Mechanism introduced in 'Grammar as a Foreign Language'  \n",
    "Orio Vinyals et al. (2015)\n",
    "\n",
    "\\begin{aligned}\n",
    "\n",
    "u_i^t &= v^T \\text{tanh}(W_1^\\prime h_i + W_2^\\prime d_t) \\\\\n",
    "\n",
    "a_i^t &= \\text{softmax}(u_i^t) \\\\\n",
    "\n",
    "d_t^\\prime &= \\sum_{i=1}^{T_A} a_i^t h_i\n",
    "\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d120b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Content-based tanh attention decoder (Vinyals et al. (2015))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1         = nn.Linear(256, 256)\n",
    "        self.W2         = nn.Linear(256, 256)\n",
    "        self.v          = nn.Linear(256, 1, bias=False)\n",
    "        self.tanh       = nn.Tanh()\n",
    "        self.softmax    = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, d, h, mask):\n",
    "        \n",
    "        # Projection of attention rnn hidden -> [B, 256]\n",
    "        d_proj = self.W1(d)\n",
    "\n",
    "        # Projection of encoder out -> [B, text_len, 256]\n",
    "        h_proj = self.W2(h)\n",
    "\n",
    "        # Expand attention rnn hidden dimension -> [B, 1, 256]\n",
    "        if d_proj.dim() == 2:\n",
    "            d_proj = d_proj.unsqueeze(1)\n",
    "\n",
    "        # Add projection results and apply tanh -> [B, text_len, 256]\n",
    "        o = self.tanh(d_proj + h_proj)\n",
    "        \n",
    "        # Calculate attention score -> [B, text_len, 1]\n",
    "        u = self.v(o)\n",
    "\n",
    "        # Squeeze output -> [B, text_len]\n",
    "        u = u.squeeze(2)\n",
    "\n",
    "        # if using masked attention\n",
    "        if mask is not None:\n",
    "            mask = mask.view(d.size(0), -1)\n",
    "            u.data.masked_fill_(mask, -float(\"inf\"))\n",
    "\n",
    "        # Convert to probability -> [B, text_len]\n",
    "        a = self.softmax(u)\n",
    "\n",
    "        # Matrix multiplication with attention score -> [B, 1, 256]\n",
    "        h_prime = torch.bmm(a.unsqueeze(1), h)\n",
    "\n",
    "        # Squeeze output -> [B, 256]\n",
    "        h_prime = h_prime.squeeze(1)\n",
    "        \n",
    "        return h_prime, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ec4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(memory, memory_lengths):\n",
    "    \"\"\"Get mask tensor from list of length using triu\n",
    "\n",
    "    Args:\n",
    "        memory: (batch, max_time, dim)\n",
    "        memory_lengths: array like\n",
    "    \"\"\"\n",
    "    max_time = memory.size(1)\n",
    "    # Create a range tensor and expand it to match the batch size\n",
    "    mask = torch.arange(max_time, device=memory.device).expand(len(memory_lengths), max_time)\n",
    "    # Compare with lengths using clone().detach() to avoid the warning\n",
    "    memory_lengths_tensor = memory_lengths.clone().detach()\n",
    "    mask = mask >= memory_lengths_tensor.unsqueeze(1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dcdca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention: content-based tanh attention decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.max_decoding_timestep = hparams.max_decoding_timestep\n",
    "\n",
    "        # prenet\n",
    "        self.n_mels = hparams.n_mels\n",
    "        self.r      = hparams.reduction_factor\n",
    "        decoder_prenet_dims = hparams.decoder_prenet\n",
    "        self.prenet = PreNet(self.n_mels * self.r, decoder_prenet_dims)\n",
    "\n",
    "        # attention rnn\n",
    "        self.attn_rnn_dim = hparams.attention_rnn_dim\n",
    "        self.attention_rnn = AttentionRNN(decoder_prenet_dims[-1], self.attn_rnn_dim)\n",
    "\n",
    "        # attention\n",
    "        self.attention = Attention()\n",
    "\n",
    "        # decoder rnn\n",
    "        decoder_dim = hp.decoder_rnn_dim\n",
    "        self.decoder_rnn = DecoderRNN(decoder_dim * 2, decoder_dim)\n",
    "\n",
    "        # postnet\n",
    "        decoder_K = hp.decoder_cbhg_k\n",
    "        decoder_cbhg_dim = hp.decoder_cbhg_dim\n",
    "        decoder_proj_dim = hp.decoder_conv1d_projection\n",
    "        highway_dims = hparams.decoder_highway\n",
    "        gru_dim = hparams.encoder_bidirectional_gru\n",
    "\n",
    "        self.postnet = CBHG(decoder_K, decoder_cbhg_dim, decoder_proj_dim, highway_dims, gru_dim)\n",
    "\n",
    "        # linears\n",
    "        self.pre_cbhg = nn.Linear(80, 128)\n",
    "        self.decoder_linear = nn.Linear(256, self.n_mels * self.r)\n",
    "        self.linear = nn.Linear(256, 1025)\n",
    "\n",
    "\n",
    "    def forward(self, z, y, len):\n",
    "        # z : encoder output        -> [B, text_len, 256]\n",
    "        # y : target melspectrogram -> [B, seq_len, 80]\n",
    "\n",
    "        # Batch size\n",
    "        B = z.size(0)\n",
    "\n",
    "        # initial variables\n",
    "        input_frame      = z.new_zeros(B, self.n_mels * self.r)\n",
    "        attn_rnn_hidden  = z.new_zeros(B, 256)\n",
    "        dec_rnn_hiddens  = [z.new_zeros(B, 256) for _ in range(2)]\n",
    "        attn_out         = z.new_zeros(B, 256)\n",
    "\n",
    "        # Store predicted melframes and alignments\n",
    "        pred_mel_frames, pred_alignments = [], []\n",
    "        \n",
    "        # Maximum timestep\n",
    "        max_T = None\n",
    "\n",
    "        # If it is training\n",
    "        if y is not None:\n",
    "            if y.size(2) == self.n_mels:\n",
    "                y = y.contiguous()\n",
    "                y = y.view(B, y.size(1) // self.r, -1)\n",
    "            assert y.size(2) == self.n_mels * self.r\n",
    "            max_T = y.size(1)\n",
    "            y = y.transpose(0, 1)\n",
    "\n",
    "        if len is not None:\n",
    "            mask = get_mask_from_lengths(z, len)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        t = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # decoder prenet -> [B, 128]\n",
    "            o = self.prenet(input_frame)\n",
    "\n",
    "            # attention rnn -> [B, 256]\n",
    "            attn_rnn_hidden = self.attention_rnn(o, attn_out, attn_rnn_hidden)\n",
    "\n",
    "            # attention -> attention context [B, 256], alignment [B, text_len]\n",
    "            attn_out, alignment = self.attention(attn_rnn_hidden, z, mask)\n",
    "            pred_alignments.append(alignment)\n",
    "\n",
    "            # decoder rnn -> decoder rnn out [B, 256], dec_rnn_hiddens [2, B, 256]\n",
    "            decoder_rnn_out, dec_rnn_hiddens = self.decoder_rnn(attn_out, attn_rnn_hidden, dec_rnn_hiddens)\n",
    "            \n",
    "            # Make reduction factor (r) number of mel frames -> [B, 80 * r]\n",
    "            r_mel_frames = self.decoder_linear(decoder_rnn_out)\n",
    "\n",
    "            pred_mel_frames.append(r_mel_frames)\n",
    "            pred_alignments.append(alignment)\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            # Inference: Check if it is end\n",
    "            if self.training is False and max_T is None:\n",
    "                if self.is_end_of_frame(r_mel_frames):\n",
    "                    break\n",
    "                elif t > self.max_decoding_timestep:\n",
    "                    print(\"Mel spectrogram does not seem to be converged.\")\n",
    "                    break\n",
    "            \n",
    "            # Training: Proceed until max timestep\n",
    "            else:\n",
    "                if t == max_T:\n",
    "                    break\n",
    "\n",
    "            # 학습: 입력을 ground truth melspectrogram frame을 사용\n",
    "            # 추론: 이전 decoder step에서 예측한 melspectrogram frame을 사용\n",
    "            input_frame = y[t - 1] if self.training else r_mel_frames\n",
    "\n",
    "\n",
    "        # Concat all pred_alignments -> [B, seq_len // r, text_len]\n",
    "        pred_alignments = torch.stack(pred_alignments).transpose(0, 1)\n",
    "\n",
    "        # Concat all mel frames -> [B, seq_len // r, 80 * r]\n",
    "        mel_pred   = torch.stack(pred_mel_frames).transpose(0, 1).contiguous()\n",
    "\n",
    "        # predicted mel spectrograms -> [B, seq_len, 80]\n",
    "        mel_pred  = mel_pred.view(B, -1, 80)\n",
    "\n",
    "        # Post-net 처리를 거친 후 선형 레이어를 통해 최종 스펙트로그램 생성\n",
    "        lin_pred  = self.pre_cbhg(mel_pred)\n",
    "        lin_pred  = self.postnet(lin_pred)\n",
    "        lin_pred  = self.linear(lin_pred)\n",
    "\n",
    "        return mel_pred, lin_pred, pred_alignments\n",
    "\n",
    "        \n",
    "    def is_end_of_frame(self, z):\n",
    "        return (z < 0.2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(Tacotron, self).__init__()\n",
    "        self.encoder = Encoder(hp)\n",
    "        self.decoder = Decoder(hp)\n",
    "\n",
    "    def forward(self, x, y=None, text_len=None):\n",
    "        x = self.encoder(x)\n",
    "        mel, lin, alignment = self.decoder(x, y, text_len)\n",
    "        return mel, lin, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacotronLoss():\n",
    "    def __init__(self):\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "\n",
    "    def __call__(self, mel_pred, lin_pred, mel_targ, lin_targ, seq_len):\n",
    "        \n",
    "        # prepare mask\n",
    "        mel_mask = self.get_mask(mel_pred, seq_len)\n",
    "        lin_mask = self.get_mask(lin_pred, seq_len)\n",
    "\n",
    "        # masking\n",
    "        mel_pred = mel_pred * mel_mask\n",
    "        lin_pred = lin_pred * lin_mask\n",
    "\n",
    "        # calculate l1 loss\n",
    "        mel_loss = self.l1_loss(mel_pred, mel_targ)\n",
    "        lin_loss = self.l1_loss(lin_pred, lin_targ)\n",
    "\n",
    "        # conjugate loss\n",
    "        return mel_loss * 0.5 + lin_loss * 0.5\n",
    "\n",
    "\n",
    "    def get_mask(self, pred, seq_len):\n",
    "        mask = pred.data.new(pred.permute(0, 2, 1).size()).fill_(1)\n",
    "        for id, len in enumerate(seq_len):\n",
    "            mask[id, :, len:] = 0\n",
    "        mask = mask.permute(0, 2, 1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e550b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Tacotron(hp).to(device)\n",
    "\n",
    "criterion = TacotronLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "dataset = TacotronDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=hp.batch_size, collate_fn=TacotronTTSCollate(), shuffle=True)\n",
    "\n",
    "epoch = 0\n",
    "for text, lin, mel, text_len, seq_len in dataloader:\n",
    "\n",
    "    # move device\n",
    "    text = text.to(device)\n",
    "    lin = lin.to(device)\n",
    "    mel = mel.to(device)\n",
    "    text_len = text_len.to(device)\n",
    "    seq_len = seq_len.to(device)\n",
    "\n",
    "    # model prediction\n",
    "    mel_pred, lin_pred, alignment = model(text, mel, text_len)\n",
    "\n",
    "    # loss function\n",
    "    loss = criterion(mel_pred, lin_pred, mel, lin, seq_len)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        show_melspectrogram(lin_pred[0].cpu().detach().numpy().transpose(1, 0),\n",
    "                            lin[0].cpu().detach().numpy().transpose(1, 0))\n",
    "\n",
    "    epoch += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
